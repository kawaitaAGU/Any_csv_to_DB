
import streamlit as st
import pandas as pd
import io
import csv
import requests
from PIL import Image
from datetime import datetime
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.pagesizes import A4
from reportlab.pdfbase.pdfmetrics import stringWidth
from reportlab.lib.utils import ImageReader
from pathlib import Path
from reportlab.pdfbase.cidfonts import UnicodeCIDFont
import re

# =========================================================
# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆIPAex ã‚’å„ªå…ˆã€ç„¡ã‘ã‚Œã°CIDãƒ•ã‚©ãƒ³ãƒˆã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
# =========================================================
def _setup_font():
    here = Path(__file__).parent
    candidates = [
        here / "fonts" / "IPAexGothic.ttf",
        here / "IPAexGothic.ttf",
        Path.cwd() / "fonts" / "IPAexGothic.ttf",
        Path.cwd() / "IPAexGothic.ttf",
    ]
    for p in candidates:
        if p.exists():
            pdfmetrics.registerFont(TTFont("Japanese", str(p)))
            return "Japanese"
    pdfmetrics.registerFont(UnicodeCIDFont("HeiseiKakuGo-W5"))
    return "HeiseiKakuGo-W5"

JAPANESE_FONT = _setup_font()

st.set_page_config(page_title="ğŸ” æ­¯ç§‘å•é¡Œæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆå…¨æ–‡æ¤œç´¢ãƒ»æœ€çµ‚å®‰å®šç‰ˆï¼‰", layout="wide")
st.title("ğŸ” æ­¯ç§‘å•é¡Œæ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆå…¨æ–‡æ¤œç´¢ãƒ»æœ€çµ‚å®‰å®šç‰ˆï¼‰")

# =========================================================
# æ–‡å­—ãƒ»æ”¹è¡Œãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================================================
def _strip(s):
    if s is None:
        return ""
    return str(s).replace("\ufeff", "").strip()

def _norm_space(s):
    # åˆ—åç”¨ï¼šç©ºç™½ãƒ»æ”¹è¡Œãƒ»ã‚¿ãƒ–ã‚’é™¤å»ï¼ˆãƒ‡ãƒ¼ã‚¿æœ¬ä½“ã«ã¯ä½¿ã‚ãªã„ï¼‰
    return re.sub(r"[\u3000 \t\r\n]+", "", _strip(s))

def _normalize_newlines(text: str, newline: str = "\n") -> str:
    if text is None:
        return ""
    t = re.sub(r"\r\n|\r", "\n", str(text))
    if newline == "\r\n":
        t = t.replace("\n", "\r\n")
    return t

def _safe_filename(s: str) -> str:
    # ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆãªã„æ–‡å­—ã‚’é™¤å»
    s = _strip(s)
    s = re.sub(r'[\\/:*?"<>|]+', "_", s)
    s = re.sub(r"\s+", "_", s)
    return s[:80] if s else "search"

# =========================================================
# åˆ—åæ­£è¦åŒ–
# =========================================================
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # åˆ—åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆåˆ—åã®ã¿ï¼‰
    df.rename(columns={c: _norm_space(c) for c in df.columns}, inplace=True)

    # ã‚ˆãã‚ã‚‹åˆ¥å â†’ æ­£å¼å
    alias = {
        "å•é¡Œæ–‡":  ["è¨­å•", "å•é¡Œ", "æœ¬æ–‡"],
        "é¸æŠè‚¢1": ["é¸æŠè‚¢ï¼¡","é¸æŠè‚¢a","A","ï½"],
        "é¸æŠè‚¢2": ["é¸æŠè‚¢ï¼¢","é¸æŠè‚¢b","B","ï½‚"],
        "é¸æŠè‚¢3": ["é¸æŠè‚¢ï¼£","é¸æŠè‚¢c","C","ï½ƒ"],
        "é¸æŠè‚¢4": ["é¸æŠè‚¢ï¼¤","é¸æŠè‚¢d","D","ï½„"],
        "é¸æŠè‚¢5": ["é¸æŠè‚¢ï¼¥","é¸æŠè‚¢e","E","ï½…"],
        "æ­£è§£":    ["è§£ç­”","ç­”ãˆ","ans","answer","æ­£ç­”"],
        "ç§‘ç›®åˆ†é¡": ["åˆ†é¡","ç§‘ç›®","ã‚«ãƒ†ã‚´ãƒª","ã‚«ãƒ†ã‚´ãƒªãƒ¼","åˆ†é‡"],
        "ãƒªãƒ³ã‚¯URL": ["ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯","ç”»åƒLink","URL","url"],
        "å•é¡Œç•ªå·ID": ["å•é¡Œç•ªå·", "è­˜åˆ¥ç•ªå·", "ID", "ç•ªå·"],
    }
    colset = set(df.columns)
    for canon, cands in alias.items():
        if canon in colset:
            continue
        for c in cands:
            if c in colset:
                df.rename(columns={c: canon}, inplace=True)
                colset.add(canon)
                break

    return df

def safe_get(row: pd.Series | dict, keys, default=""):
    if isinstance(row, pd.Series):
        row = row.to_dict()
    for k in keys:
        if k in row:
            v = row.get(k)
            try:
                if pd.isna(v):
                    continue
            except Exception:
                pass
            s = str(v) if v is not None else ""
            s = s.strip()
            if s != "":
                return s
    return default

def ensure_output_columns(df: pd.DataFrame) -> pd.DataFrame:
    need = [
        "å•é¡Œæ–‡","é¸æŠè‚¢1","é¸æŠè‚¢2","é¸æŠè‚¢3","é¸æŠè‚¢4","é¸æŠè‚¢5",
        "æ­£è§£","ç§‘ç›®åˆ†é¡","å•é¡Œç•ªå·ID","ãƒªãƒ³ã‚¯URL"
    ]
    out = df.copy()
    for c in need:
        if c not in out.columns:
            out[c] = ""
    return out

# =========================================================
# â˜… æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’ã€Œæ—¥æœ¬èªã¨ã—ã¦è‡ªç„¶ã«èª­ã‚ã‚‹ã‚‚ã®ã€ã«å¯„ã›ã‚‹ãƒ‡ã‚³ãƒ¼ãƒ‰
#    ï¼ˆcp932æ··åœ¨/ä¸€éƒ¨ä¸æ­£ãƒã‚¤ãƒˆã§ã‚‚â€œæ–‡å­—åŒ–ã‘å„ªå…ˆâ€ã‚’é¿ã‘ã‚‹ï¼‰
# =========================================================
def _decode_best_effort(raw: bytes) -> str:
    candidates = ["utf-8-sig", "utf-8", "cp932", "shift_jis", "euc_jp"]

    # ã¾ãšã¯ strict decode ã‚’è©¦ã™
    for enc in candidates:
        try:
            return raw.decode(enc)
        except Exception:
            pass

    # strict ãŒå…¨æ»…ã—ãŸå ´åˆï¼šreplace decode ã—ã¦ã€Œæ—¥æœ¬èªãŒå¤šãã€ï¿½ãŒå°‘ãªã„ã€ã‚‚ã®ã‚’æ¡ç”¨
    best_text = None
    best_score = None

    # ã“ã“ãŒé‡è¦ï¼šcp932 ç³»ã‚’å„ªå…ˆã—ã¦è©•ä¾¡ï¼ˆutf-8 ignore ã§â€œèª­ã‚ãŸæ‰±ã„â€ã‚’é˜²ãï¼‰
    for enc in ["cp932", "shift_jis", "euc_jp", "utf-8-sig", "utf-8"]:
        try:
            t = raw.decode(enc, errors="replace")
        except Exception:
            continue

        rep = t.count("\ufffd")  # replacement char
        jp = sum(
            1 for ch in t
            if ("\u3040" <= ch <= "\u30ff") or ("\u4e00" <= ch <= "\u9fff")
        )
        # rep ãŒå°‘ãªã„ã»ã©è‰¯ã„ / jp ãŒå¤šã„ã»ã©è‰¯ã„
        score = rep * 1000 - jp
        if best_score is None or score < best_score:
            best_score = score
            best_text = t

    if best_text is not None:
        return best_text

    # æœ€å¾Œã®æ‰‹æ®µ
    return raw.decode("utf-8", errors="ignore")

# =========================================================
# â˜… CSVèª­ã¿è¾¼ã¿ï¼ˆãƒ˜ãƒƒãƒ€è£œæ­£ï¼‹åˆ—æ•°è£œæ­£ã¤ãï¼‰
# =========================================================
def read_csv_safely_with_column_fix(uploaded_file) -> pd.DataFrame:
    """
    1) bytesâ†’æ–‡å­—åˆ—ï¼ˆæ—¥æœ¬èªã¨ã—ã¦è‡ªç„¶ã«èª­ã‚ã‚‹ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’æ¡ç”¨ï¼‰
    2) ãƒ˜ãƒƒãƒ€1è¡Œç›®ã® 'ã€' ã‚’ ',' ã«è£œæ­£ï¼ˆæ··åœ¨å¯¾ç­–ï¼‰
    3) csv.readerã§è¡Œã”ã¨ã«åˆ—æ•°ã‚’ãƒ˜ãƒƒãƒ€ã«åˆã‚ã›ã‚‹
       - åˆ—ä¸è¶³â†’å³ã‚’ç©ºã§åŸ‹ã‚ã‚‹
       - åˆ—éå¤šâ†’å…ˆé ­åˆ—ã¸å¸åï¼ˆæœ¬æ–‡ã«ã‚«ãƒ³ãƒãŒå…¥ã£ã¦ã‚‚å£Šã‚Œã«ãã„ï¼‰
    """
    raw = uploaded_file.getvalue()
    text = _decode_best_effort(raw)

    lines = text.splitlines()
    if not lines:
        return pd.DataFrame()

    # â˜…ãƒ˜ãƒƒãƒ€ã ã‘è£œæ­£ï¼ˆã“ã“ãŒåŠ¹ãï¼šå•é¡Œæ–‡ã€é¸æŠè‚¢1,... ã®ã€Œã€ã€æ··åœ¨ï¼‰
    header_line = lines[0].replace("ã€", ",")
    data_lines = lines[1:]

    header = next(csv.reader([header_line]))
    header = [h.strip().replace("\ufeff", "") for h in header]
    ncol = len(header)

    fixed_rows = []
    reader = csv.reader(data_lines)
    for row in reader:
        if not row or all((c.strip() == "" for c in row)):
            continue

        # åˆ—ãŒå¤šã™ãã‚‹ï¼šä½™ã‚Šã‚’å…ˆé ­åˆ—ã¸å¸å
        while len(row) > ncol:
            row[0] = row[0] + "," + row[1]
            del row[1]

        # åˆ—ãŒè¶³ã‚Šãªã„ï¼šå³ã‚’ç©ºã§åŸ‹ã‚ã‚‹
        if len(row) < ncol:
            row = row + [""] * (ncol - len(row))

        fixed_rows.append(row)

    df = pd.DataFrame(fixed_rows, columns=header).fillna("")
    return df

# =========================================================
# å…¥åŠ›CSVï¼ˆDrag & Dropï¼‰
# =========================================================
uploaded = st.file_uploader("CSVã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ï¼ˆã¾ãŸã¯é¸æŠï¼‰", type=["csv"])
if uploaded is None:
    st.info("ä»»æ„ã®CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆãƒ˜ãƒƒãƒ€ã‚ã‚Šæ¨å¥¨ï¼‰ã€‚")
    st.stop()

df = read_csv_safely_with_column_fix(uploaded)
df = normalize_columns(df)

# URLåˆ—ã ã‘ã®CSVã§ã‚‚ã€Œãƒªãƒ³ã‚¯URLã€ã«å¯„ã›ã‚‹
if "ãƒªãƒ³ã‚¯URL" not in df.columns and "URL" in df.columns:
    df.rename(columns={"URL": "ãƒªãƒ³ã‚¯URL"}, inplace=True)

# =========================================================
# æ¤œç´¢
# =========================================================
query = st.text_input("æ¤œç´¢èªï¼ˆä¾‹ï¼šã©ã‚Œã‹ / ã‚¨ãƒŠãƒ¡ãƒ«è³ª / 1459ï¼‰")
st.caption("ğŸ’¡ `&` ã§ANDæ¤œç´¢ï¼ˆä¾‹: ãƒ¬ã‚¸ãƒ³ & ç†å·¥ï¼‰")

if not query:
    st.stop()

keywords = [kw.strip() for kw in query.split("&") if kw.strip()]

def row_text(r: pd.Series) -> str:
    # å…¨åˆ—ã‚’å¯¾è±¡ã«ã™ã‚‹ï¼ˆåˆ—åãŒæƒ³å®šå¤–ã§ã‚‚æ¤œç´¢æ¼ã‚Œã—ãªã„ï¼‰
    vals = []
    for v in r.values:
        if v is None:
            continue
        s = str(v)
        if s.strip() == "":
            continue
        vals.append(s)
    return " ".join(vals)

def match_all_keywords(row: pd.Series) -> bool:
    text = row_text(row).casefold()
    return all(kw.casefold() in text for kw in keywords)

df_filtered = df[df.apply(match_all_keywords, axis=1)].reset_index(drop=True)
st.info(f"{len(df_filtered)}ä»¶ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸ")

timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
file_prefix = f"{_safe_filename(query)}_{timestamp}"

# =========================================================
# CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =========================================================
csv_buffer = io.StringIO()
ensure_output_columns(df_filtered).to_csv(csv_buffer, index=False)
st.download_button(
    label="ğŸ“¥ ãƒ’ãƒƒãƒˆçµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=csv_buffer.getvalue(),
    file_name=f"{file_prefix}.csv",
    mime="text/csv"
)

# =========================================================
# GoodNotes ç”¨ CSVï¼ˆFront/Backï¼‰
# =========================================================
def _gn_clean(s: str) -> str:
    # å•é¡Œæ–‡æœ«å°¾ã®è­˜åˆ¥ç•ªå·ã¯ä¿æŒï¼ˆå…¨è§’ç©ºç™½ã ã‘é™¤å»ï¼‰
    return _strip(s).replace("ã€€", "")

def _gn_make_front_back(row: pd.Series,
                        numbering: str = "ABC",
                        add_labels: bool = True,
                        add_meta: bool = True) -> tuple[str, str]:
    q = _gn_clean(safe_get(row, ["å•é¡Œæ–‡"]))
    choices = [
        _gn_clean(safe_get(row, ["é¸æŠè‚¢1"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢2"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢3"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢4"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢5"])),
    ]
    labels = ["A","B","C","D","E"] if numbering == "ABC" else ["1","2","3","4","5"]
    choice_lines = [f"{labels[i]}. {_normalize_newlines(txt)}" for i, txt in enumerate(choices) if txt]

    front = _normalize_newlines(q)
    if choice_lines:
        front = front + "\n\n" + "\n".join(choice_lines)

    ans = _gn_clean(safe_get(row, ["æ­£è§£"]))
    back = f"æ­£è§£: {ans}" if add_labels else ans

    if add_meta:
        cat = _gn_clean(safe_get(row, ["ç§‘ç›®åˆ†é¡"]))
        code = _gn_clean(safe_get(row, ["å•é¡Œç•ªå·ID"]))
        extra = "\n".join([s for s in (cat, code) if s])
        if extra:
            back = back + "\n\n" + _normalize_newlines(extra)

    back = _normalize_newlines(back)
    return front, back

def dataframe_to_goodnotes_bytes(df_in: pd.DataFrame) -> bytes:
    base = ensure_output_columns(df_in)
    fronts, backs = [], []
    for _, row in base.iterrows():
        f, b = _gn_make_front_back(row)
        fronts.append(f); backs.append(b)
    out = pd.DataFrame({"Front": fronts, "Back": backs})
    for c in out.columns:
        out[c] = out[c].map(lambda v: _normalize_newlines(v, "\n"))
    buf = io.StringIO()
    buf.write("\ufeff")  # BOM
    out.to_csv(buf, index=False, lineterminator="\n")
    return buf.getvalue().encode("utf-8")

st.download_button(
    label="ğŸ“¥ GoodNotesç”¨CSVï¼ˆFront/Backï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=dataframe_to_goodnotes_bytes(df_filtered),
    file_name=f"{file_prefix}_goodnotes.csv",
    mime="text/csv",
)

# =========================================================
# TXT / PDF å…±é€š
# =========================================================
def convert_google_drive_link(url):
    if "drive.google.com" in url and "/file/d/" in url:
        try:
            file_id = url.split("/file/d/")[1].split("/")[0]
            return f"https://drive.google.com/uc?export=view&id={file_id}"
        except Exception:
            return url
    return url

def wrap_text(text: str, max_width: float, font_name: str, font_size: int):
    s = "" if text is None else str(text)
    if s == "":
        return [""]
    lines, buf = [], ""
    for ch in s:
        if stringWidth(buf + ch, font_name, font_size) <= max_width:
            buf += ch
        else:
            lines.append(buf)
            buf = ch
    if buf:
        lines.append(buf)
    return lines

def wrapped_lines(prefix: str, value: str, usable_width: float, font: str, size: int):
    return wrap_text(f"{prefix}{value}", usable_width, font, size)

def format_record_to_text(row: pd.Series) -> str:
    q = safe_get(row, ["å•é¡Œæ–‡"])
    parts = [f"å•é¡Œæ–‡: {q}"]
    for i in range(1, 6):
        choice = safe_get(row, [f"é¸æŠè‚¢{i}"])
        if choice:
            parts.append(f"é¸æŠè‚¢{i}: {choice}")
    parts.append(f"æ­£è§£: {safe_get(row, ['æ­£è§£'])}")
    parts.append(f"åˆ†é¡: {safe_get(row, ['ç§‘ç›®åˆ†é¡'])}")
    code = safe_get(row, ["å•é¡Œç•ªå·ID"])
    if code:
        parts.append(f"å•é¡Œç•ªå·: {code}")
    link = safe_get(row, ["ãƒªãƒ³ã‚¯URL"])
    if link:
        parts.append(f"ç”»åƒãƒªãƒ³ã‚¯: {convert_google_drive_link(link)}ï¼ˆPDFã«ç”»åƒè¡¨ç¤ºï¼‰")
    return "\n".join(parts)

# =========================================================
# TXT ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =========================================================
txt_buffer = io.StringIO()
for _, row in df_filtered.iterrows():
    txt_buffer.write(format_record_to_text(row))
    txt_buffer.write("\n\n" + "-"*40 + "\n\n")
st.download_button(
    label="ğŸ“„ ãƒ’ãƒƒãƒˆçµæœã‚’TEXTãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=txt_buffer.getvalue(),
    file_name=f"{file_prefix}.txt",
    mime="text/plain"
)

# =========================================================
# PDF ä½œæˆ
# =========================================================
def create_pdf(records: pd.DataFrame):
    pdf_buffer = io.BytesIO()
    c = canvas.Canvas(pdf_buffer, pagesize=A4)
    c.setFont(JAPANESE_FONT, 12)
    width, height = A4

    top_margin, bottom_margin = 40, 60
    left_margin, right_margin = 40, 40
    usable_width = width - left_margin - right_margin
    page_usable_h = (height - top_margin) - bottom_margin
    line_h = 18
    y = height - top_margin

    def new_page():
        nonlocal y
        c.showPage()
        c.setFont(JAPANESE_FONT, 12)
        y = height - top_margin

    def draw_wrapped_lines(lines):
        nonlocal y
        for ln in lines:
            c.drawString(left_margin, y, ln)
            y -= line_h

    for _, row in records.iterrows():
        q = safe_get(row, ["å•é¡Œæ–‡"])

        choices = []
        for i in range(1, 6):
            v = safe_get(row, [f"é¸æŠè‚¢{i}"])
            if v:
                choices.append((i, v))

        ans = safe_get(row, ["æ­£è§£"])
        cat = safe_get(row, ["ç§‘ç›®åˆ†é¡"])
        code = safe_get(row, ["å•é¡Œç•ªå·ID"])

        # ç”»åƒã®äº‹å‰å–å¾—
        pil = None
        img_est_h = 0
        link_raw = safe_get(row, ["ãƒªãƒ³ã‚¯URL"])
        if link_raw:
            try:
                image_url = convert_google_drive_link(link_raw)
                resp = requests.get(image_url, timeout=5)
                pil = Image.open(io.BytesIO(resp.content)).convert("RGB")
                iw, ih = pil.size
                scale = min(usable_width / iw, page_usable_h / ih, 1.0)
                nw, nh = iw * scale, ih * scale
                img_est_h = nh + 20
            except Exception:
                pil = None
                img_est_h = len(wrapped_lines("", "[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—]", usable_width, JAPANESE_FONT, 12)) * line_h

        # é«˜ã•è¦‹ç©ã‚Š
        est_h = 0
        q_lines = wrapped_lines("å•é¡Œæ–‡: ", q, usable_width, JAPANESE_FONT, 12)
        est_h += len(q_lines) * line_h
        choice_lines_list = []
        for i, v in choices:
            ls = wrapped_lines(f"é¸æŠè‚¢{i}: ", v, usable_width, JAPANESE_FONT, 12)
            choice_lines_list.append(ls)
            est_h += len(ls) * line_h
        est_h += img_est_h if img_est_h else 0
        ans_lines = wrapped_lines("æ­£è§£: ", ans, usable_width, JAPANESE_FONT, 12)
        cat_lines = wrapped_lines("åˆ†é¡: ", cat, usable_width, JAPANESE_FONT, 12)
        code_lines = wrapped_lines("å•é¡Œç•ªå·: ", code, usable_width, JAPANESE_FONT, 12)
        est_h += (len(ans_lines) + len(cat_lines) + len(code_lines)) * line_h + 20

        if y - est_h < bottom_margin:
            new_page()

        draw_wrapped_lines(q_lines)
        for ls in choice_lines_list:
            draw_wrapped_lines(ls)

        if pil is not None:
            try:
                iw, ih = pil.size
                scale = min(usable_width / iw, page_usable_h / ih, 1.0)
                nw, nh = iw * scale, ih * scale
                if y - nh < bottom_margin:
                    new_page()
                remaining = y - bottom_margin
                if nh > remaining:
                    adj = remaining / nh
                    nw, nh = nw * adj, nh * adj
                img_io = io.BytesIO()
                pil.save(img_io, format="PNG")
                img_io.seek(0)
                img_reader = ImageReader(img_io)
                c.drawImage(img_reader, left_margin, y - nh, width=nw, height=nh,
                            preserveAspectRatio=True, mask='auto')
                y -= nh + 20
            except Exception as e:
                err_lines = wrapped_lines("", f"[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {e}]", usable_width, JAPANESE_FONT, 12)
                draw_wrapped_lines(err_lines)
        else:
            if link_raw:
                draw_wrapped_lines(wrapped_lines("", "[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—]", usable_width, JAPANESE_FONT, 12))

        draw_wrapped_lines(ans_lines)
        draw_wrapped_lines(cat_lines)
        draw_wrapped_lines(code_lines)

        if y - 20 < bottom_margin:
            new_page()
        else:
            y -= 20

    c.save()
    pdf_buffer.seek(0)
    return pdf_buffer.getvalue()

if "pdf_bytes" not in st.session_state:
    st.session_state["pdf_bytes"] = None

if st.button("ğŸ–¨ï¸ PDFã‚’ä½œæˆï¼ˆç”»åƒä»˜ãï¼‰"):
    with st.spinner("PDFã‚’ä½œæˆä¸­â€¦"):
        st.session_state["pdf_bytes"] = create_pdf(df_filtered)
    st.success("âœ… PDFä½œæˆå®Œäº†ï¼")

if st.session_state["pdf_bytes"] is not None:
    st.download_button(
        label="ğŸ“„ ãƒ’ãƒƒãƒˆçµæœã‚’PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=st.session_state["pdf_bytes"],
        file_name=f"{file_prefix}.pdf",
        mime="application/pdf"
    )

# =========================================================
# ç”»é¢ã®ä¸€è¦§ï¼ˆå•é¡Œæ–‡æœ«å°¾ã®è­˜åˆ¥ç•ªå·ã‚’åˆ‡ã‚‰ãªã„ï¼‰
# =========================================================
st.markdown("### ğŸ” ãƒ’ãƒƒãƒˆã—ãŸå•é¡Œä¸€è¦§")
for i, (_, record) in enumerate(df_filtered.iterrows()):
    title = safe_get(record, ["å•é¡Œæ–‡"])
    with st.expander(f"{i+1}. {title}"):
        st.markdown("### ğŸ“ å•é¡Œæ–‡")
        st.write(title)

        st.markdown("### âœï¸ é¸æŠè‚¢")
        for j in range(1, 6):
            val = safe_get(record, [f"é¸æŠè‚¢{j}"])
            if val:
                st.write(f"- {val}")

        show_ans = st.checkbox("æ­£è§£ã‚’è¡¨ç¤ºã™ã‚‹", key=f"show_answer_{i}", value=False)
        if show_ans:
            st.markdown(f"**âœ… æ­£è§£:** {safe_get(record, ['æ­£è§£'])}")
        else:
            st.markdown("**âœ… æ­£è§£:** |||ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§è¡¨ç¤ºï¼‰|||")

        st.markdown(f"**ğŸ“š åˆ†é¡:** {safe_get(record, ['ç§‘ç›®åˆ†é¡'])}")

        code = safe_get(record, ["å•é¡Œç•ªå·ID"])
        if code:
            st.markdown(f"**ğŸ†” å•é¡Œç•ªå·:** {code}")

        link = safe_get(record, ["ãƒªãƒ³ã‚¯URL"])
        if link:
            st.markdown(f"[ç”»åƒãƒªãƒ³ã‚¯ã¯ã“ã¡ã‚‰]({convert_google_drive_link(link)})")
        else:
            st.write("ï¼ˆç”»åƒãƒªãƒ³ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰")
