import streamlit as st
import pandas as pd
import io
import requests
from PIL import Image
from datetime import datetime
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.pagesizes import A4
from reportlab.pdfbase.pdfmetrics import stringWidth
from reportlab.lib.utils import ImageReader
import time
from pathlib import Path
from reportlab.pdfbase.cidfonts import UnicodeCIDFont
import re

# ---- ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆIPAex ã‚’å„ªå…ˆã€ç„¡ã‘ã‚Œã°CIDãƒ•ã‚©ãƒ³ãƒˆã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰----
def _setup_font():
    here = Path(__file__).parent
    candidates = [
        here / "fonts" / "IPAexGothic.ttf",
        here / "IPAexGothic.ttf",
        Path.cwd() / "fonts" / "IPAexGothic.ttf",
        Path.cwd() / "IPAexGothic.ttf",
    ]
    for p in candidates:
        if p.exists():
            pdfmetrics.registerFont(TTFont("Japanese", str(p)))
            return "Japanese"
    pdfmetrics.registerFont(UnicodeCIDFont("HeiseiKakuGo-W5"))
    return "HeiseiKakuGo-W5"

JAPANESE_FONT = _setup_font()

st.set_page_config(page_title="ğŸ” å­¦ç”ŸæŒ‡å°ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", layout="wide")
st.title("ğŸ” å­¦ç”ŸæŒ‡å°ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹")

# ===== æ–‡å­—ãƒ»æ”¹è¡Œãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def _strip(s):
    if s is None:
        return ""
    return str(s).replace("\ufeff", "").strip()

def _norm_space(s):
    return re.sub(r"[\u3000 \t\r\n]+", "", _strip(s))

def _normalize_newlines(text: str, newline: str = "\n") -> str:
    if text is None:
        return ""
    t = re.sub(r"\r\n|\r", "\n", str(text))
    if newline == "\r\n":
        t = t.replace("\n", "\r\n")
    return t

# ===== åˆ—åæ­£è¦åŒ– & å–ã‚Šã“ã¼ã—æ•‘æ¸ˆ =====
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """BOM/ç©ºç™½/æ”¹è¡Œã‚’é™¤å»ã—ã€ã‚ˆãã‚ã‚‹åˆ¥åã‚’æ­£å¼åã¸å¯„ã›ã‚‹ã€‚
       ã•ã‚‰ã« 'A18æ­¯ç§‘ç†å·¥å­¦' ã®ã‚ˆã†ãª é€£çµåˆ— â†’ å•é¡Œç•ªå·ID/ç§‘ç›®åˆ†é¡ ã‚’è‡ªå‹•æŠ½å‡ºã€‚"""
    df = df.copy()
    # åˆ—åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    orig_to_clean = {c: _norm_space(c) for c in df.columns}
    df.rename(columns=orig_to_clean, inplace=True)

    # ã‚ˆãã‚ã‚‹åˆ¥å â†’ æ­£å¼å
    alias = {
        "å•é¡Œæ–‡":  ["è¨­å•", "å•é¡Œ", "æœ¬æ–‡"],
        "é¸æŠè‚¢1": ["é¸æŠè‚¢ï¼¡","é¸æŠè‚¢a","A","ï½"],
        "é¸æŠè‚¢2": ["é¸æŠè‚¢ï¼¢","é¸æŠè‚¢b","B","ï½‚"],
        "é¸æŠè‚¢3": ["é¸æŠè‚¢ï¼£","é¸æŠè‚¢c","C","ï½ƒ"],
        "é¸æŠè‚¢4": ["é¸æŠè‚¢ï¼¤","é¸æŠè‚¢d","D","ï½„"],
        "é¸æŠè‚¢5": ["é¸æŠè‚¢ï¼¥","é¸æŠè‚¢e","E","ï½…"],
        "æ­£è§£":    ["è§£ç­”","ç­”ãˆ","ans","answer","æ­£ç­”"],
        "ç§‘ç›®åˆ†é¡": ["åˆ†é¡","ç§‘ç›®","ã‚«ãƒ†ã‚´ãƒª","ã‚«ãƒ†ã‚´ãƒªãƒ¼"],
        "ãƒªãƒ³ã‚¯URL": ["ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯","ç”»åƒLink","URL","url"],
    }
    colset = set(df.columns)
    for canon, cands in alias.items():
        if canon in colset:  # æ—¢ã«ã‚ã‚‹
            continue
        for c in cands:
            if c in colset:
                df.rename(columns={c: canon}, inplace=True)
                colset.add(canon)
                break

    # --- å–ã‚Šã“ã¼ã—æ•‘æ¸ˆï¼šå•é¡Œç•ªå·ï¼‹åˆ†é¡ãŒé€£çµã•ã‚ŒãŸåˆ—ã‚’æŠ½å‡º ---
    # 1) ãƒ˜ãƒƒãƒ€ãƒ¼ã« 'å•é¡Œç•ªå·' ã‚’å«ã‚€åˆ—
    candidate_cols = [c for c in df.columns if "å•é¡Œç•ªå·" in c]
    # 2) å€¤ãŒ 'A12æ­¯ç§‘ç†å·¥å­¦' ã®ã‚ˆã†ãªã€Œè‹±å­—+æ•°å­—+æ—¥æœ¬èªã€ã®åˆ—
    if not candidate_cols:
        for c in df.columns:
            series = df[c].astype(str)
            if series.str.contains(r"^[A-Za-z]\d+[\u4e00-\u9fff\u3040-\u30ff]+", regex=True, na=False).any():
                candidate_cols.append(c)
                break

    # å•é¡Œç•ªå·ID/ç§‘ç›®åˆ†é¡ã®æŠ½å‡ºï¼ˆæ—¢å­˜åˆ—ãŒãªã‘ã‚Œã°ä½œã‚‹ï¼‰
    if candidate_cols:
        src = candidate_cols[0]
        # ã‚‚ã—ãƒ˜ãƒƒãƒ€ãƒ¼è‡ªä½“ã« â€œA00æ­¯ç§‘ç†å·¥å­¦â€ ãªã©ãŒåŸ‹ã¾ã£ã¦ã„ã‚Œã°ã€ãã“ã‹ã‚‰æ—¢å®šåˆ†é¡ã‚’æ‹¾ã†
        m_head = re.search(r"([A-Za-z]\d+)?([\u4e00-\u9fff\u3040-\u30ff]+)", src)
        default_cat = m_head.group(2) if m_head else ""

        def split_code_cat(val: str):
            s = _strip(val)
            if not s:
                return "", default_cat
            m = re.match(r"([A-Za-z]\d+)([\u4e00-\u9fff\u3040-\u30ff]+)", s)
            if m:
                return m.group(1), m.group(2)
            # æ•°å­—ï¼‹æ—¥æœ¬èªã®ã‚±ãƒ¼ã‚¹ï¼ˆä¾‹ï¼š18æ­¯ç§‘ç†å·¥å­¦ï¼‰
            m2 = re.match(r"([0-9]+)([\u4e00-\u9fff\u3040-\u30ff]+)", s)
            if m2:
                return m2.group(1), m2.group(2)
            # æ—¥æœ¬èªã®ã¿ or ãã‚Œä»¥å¤–
            return "", s

        if "å•é¡Œç•ªå·ID" not in df.columns or "ç§‘ç›®åˆ†é¡" not in df.columns:
            codes = []
            cats = []
            for v in df[src].astype(str):
                code, cat = split_code_cat(v)
                codes.append(code)
                cats.append(cat)
            if "å•é¡Œç•ªå·ID" not in df.columns:
                df["å•é¡Œç•ªå·ID"] = codes
            if "ç§‘ç›®åˆ†é¡" not in df.columns:
                df["ç§‘ç›®åˆ†é¡"] = cats

    return df

def safe_get(row: pd.Series | dict, keys, default=""):
    """Series/è¾æ›¸ã‹ã‚‰å®‰å…¨ã«å€¤ã‚’å–å¾—ï¼ˆNaN, ç©ºç™½, åˆ¥åã‚’è€ƒæ…®ï¼‰"""
    if isinstance(row, pd.Series):
        row = row.to_dict()
    for k in keys:
        if k in row:
            v = row.get(k)
            try:
                if pd.isna(v):
                    continue
            except Exception:
                pass
            s = str(v).strip() if v is not None else ""
            if s:
                return s
    return default

def ensure_output_columns(df: pd.DataFrame) -> pd.DataFrame:
    need = [
        "å•é¡Œæ–‡","é¸æŠè‚¢1","é¸æŠè‚¢2","é¸æŠè‚¢3","é¸æŠè‚¢4","é¸æŠè‚¢5",
        "æ­£è§£","ç§‘ç›®åˆ†é¡","å•é¡Œç•ªå·ID","ãƒªãƒ³ã‚¯URL"
    ]
    out = df.copy()
    for c in need:
        if c not in out.columns:
            out[c] = ""
    return out

# ======================
# å…¥åŠ›CSVï¼ˆDrag & Dropï¼‰
# ======================
uploaded = st.file_uploader("CSVã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ï¼ˆã¾ãŸã¯é¸æŠï¼‰", type=["csv"])
if uploaded is None:
    st.info("ä»»æ„ã®CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆãƒ˜ãƒƒãƒ€ã‚ã‚Šæ¨å¥¨ï¼‰ã€‚")
    st.stop()

try:
    df = pd.read_csv(uploaded, dtype=str, encoding="utf-8-sig").fillna("")
except Exception:
    # æ–‡å­—ã‚³ãƒ¼ãƒ‰é•ã„ãªã©ã®ä¿é™º
    df = pd.read_csv(uploaded, dtype=str, encoding="utf-8", errors="ignore").fillna("")
df = normalize_columns(df)

# ===== æ¤œç´¢ =====
query = st.text_input("å•é¡Œæ–‡ãƒ»é¸æŠè‚¢ãƒ»åˆ†é¡ãƒ»å•é¡Œç•ªå·ã§æ¤œç´¢:")
st.caption("ğŸ’¡ æ¤œç´¢èªã‚’ `&` ã§ã¤ãªã’ã‚‹ã¨ANDæ¤œç´¢ï¼ˆä¾‹: ãƒ¬ã‚¸ãƒ³ & ç†å·¥ï¼‰")

if not query:
    st.stop()

keywords = [kw.strip() for kw in query.split("&") if kw.strip()]

def row_text(r: pd.Series) -> str:
    parts = [
        safe_get(r, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"]),
        *[safe_get(r, [f"é¸æŠè‚¢{i}"]) for i in range(1,6)],
        safe_get(r, ["æ­£è§£","æ­£ç­”","è§£ç­”","ç­”ãˆ"]),
        safe_get(r, ["ç§‘ç›®åˆ†é¡","åˆ†é¡","ç§‘ç›®"]),
        safe_get(r, ["å•é¡Œç•ªå·ID","å•é¡Œç•ªå·"]),
        safe_get(r, ["ãƒªãƒ³ã‚¯URL","URL","ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯"]),
    ]
    return " ".join([p for p in parts if p])

df_filtered = df[df.apply(
    lambda row: all(kw.lower() in row_text(row).lower() for kw in keywords),
    axis=1
)]
df_filtered = df_filtered.reset_index(drop=True)

st.info(f"{len(df_filtered)}ä»¶ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸ")

timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
file_prefix = f"{(query if query else 'æ¤œç´¢ãªã—')}{timestamp}"

# ===== CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ =====
csv_buffer = io.StringIO()
ensure_output_columns(df_filtered).to_csv(csv_buffer, index=False)
st.download_button(
    label="ğŸ“¥ ãƒ’ãƒƒãƒˆçµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=csv_buffer.getvalue(),
    file_name=f"{file_prefix}.csv",
    mime="text/csv"
)

# ===== GoodNotes ç”¨ CSVï¼ˆFront/Backï¼‰ =====
def _gn_clean(s: str) -> str:
    return _strip(s).replace("ã€€", "")

def _gn_make_front_back(row: pd.Series,
                        numbering: str = "ABC",
                        add_labels: bool = True,
                        add_meta: bool = True) -> tuple[str, str]:
    q = _gn_clean(safe_get(row, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"]))
    choices = [
        _gn_clean(safe_get(row, ["é¸æŠè‚¢1"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢2"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢3"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢4"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢5"])),
    ]
    labels = ["A","B","C","D","E"] if numbering == "ABC" else ["1","2","3","4","5"]
    choice_lines = [f"{labels[i]}. {_normalize_newlines(txt)}" for i, txt in enumerate(choices) if txt]

    front = _normalize_newlines(q)
    if choice_lines:
        front = front + "\n\n" + "\n".join(choice_lines)

    ans = _gn_clean(safe_get(row, ["æ­£è§£","æ­£ç­”","è§£ç­”","ç­”ãˆ"]))
    back = f"æ­£è§£: {ans}" if add_labels else ans

    if add_meta:
        cat = _gn_clean(safe_get(row, ["ç§‘ç›®åˆ†é¡","åˆ†é¡","ç§‘ç›®"]))
        code = _gn_clean(safe_get(row, ["å•é¡Œç•ªå·ID","å•é¡Œç•ªå·"]))
        extra = "\n".join([s for s in (cat, code) if s])
        if extra:
            back = back + "\n\n" + _normalize_newlines(extra)

    back = _normalize_newlines(back)
    return front, back

def dataframe_to_goodnotes_bytes(df: pd.DataFrame) -> bytes:
    base = ensure_output_columns(df)
    fronts, backs = [], []
    for _, row in base.iterrows():
        f, b = _gn_make_front_back(row)
        fronts.append(f); backs.append(b)
    out = pd.DataFrame({"Front": fronts, "Back": backs})
    # ã‚»ãƒ«å†…éƒ¨æ”¹è¡Œã¯LF
    for c in out.columns:
        out[c] = out[c].map(lambda v: _normalize_newlines(v, "\n"))
    buf = io.StringIO()
    buf.write("\ufeff")  # BOM
    out.to_csv(buf, index=False, lineterminator="\n")
    return buf.getvalue().encode("utf-8")

st.download_button(
    label="ğŸ“¥ GoodNotesç”¨CSVï¼ˆFront/Backï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=dataframe_to_goodnotes_bytes(df_filtered),
    file_name=f"{file_prefix}_goodnotes.csv",
    mime="text/csv",
)

# ===== TXT æ•´å½¢ =====
def convert_google_drive_link(url):
    if "drive.google.com" in url and "/file/d/" in url:
        try:
            file_id = url.split("/file/d/")[1].split("/")[0]
            return f"https://drive.google.com/uc?export=view&id={file_id}"
        except Exception:
            return url
    return url

def wrap_text(text: str, max_width: float, font_name: str, font_size: int):
    s = "" if text is None else str(text)
    if s == "":
        return [""]
    lines, buf = [], ""
    for ch in s:
        if stringWidth(buf + ch, font_name, font_size) <= max_width:
            buf += ch
        else:
            lines.append(buf)
            buf = ch
    if buf:
        lines.append(buf)
    return lines

def wrapped_lines(prefix: str, value: str, usable_width: float, font: str, size: int):
    return wrap_text(f"{prefix}{value}", usable_width, font, size)

def format_record_to_text(row: pd.Series) -> str:
    q = safe_get(row, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"])
    parts = [f"å•é¡Œæ–‡: {q}"]
    for i in range(1, 6):
        choice = safe_get(row, [f"é¸æŠè‚¢{i}"])
        if choice:
            parts.append(f"é¸æŠè‚¢{i}: {choice}")
    parts.append(f"æ­£è§£: {safe_get(row, ['æ­£è§£','æ­£ç­”','è§£ç­”','ç­”ãˆ'])}")
    parts.append(f"åˆ†é¡: {safe_get(row, ['ç§‘ç›®åˆ†é¡','åˆ†é¡','ç§‘ç›®'])}")
    code = safe_get(row, ["å•é¡Œç•ªå·ID","å•é¡Œç•ªå·"])
    if code:
        parts.append(f"å•é¡Œç•ªå·: {code}")
    link = safe_get(row, ["ãƒªãƒ³ã‚¯URL","ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯","URL"])
    if link:
        parts.append(f"ç”»åƒãƒªãƒ³ã‚¯: {convert_google_drive_link(link)}ï¼ˆPDFã«ç”»åƒè¡¨ç¤ºï¼‰")
    return "\n".join(parts)

# ===== TXT ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ =====
txt_buffer = io.StringIO()
for _, row in df_filtered.iterrows():
    txt_buffer.write(format_record_to_text(row))
    txt_buffer.write("\n\n" + "-"*40 + "\n\n")
st.download_button(
    label="ğŸ“„ ãƒ’ãƒƒãƒˆçµæœã‚’TEXTãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=txt_buffer.getvalue(),
    file_name=f"{file_prefix}.txt",
    mime="text/plain"
)

# ===== PDF ä½œæˆ =====
def create_pdf(records, progress=None, status=None, start_time=None):
    pdf_buffer = io.BytesIO()
    c = canvas.Canvas(pdf_buffer, pagesize=A4)
    c.setFont(JAPANESE_FONT, 12)
    width, height = A4

    top_margin, bottom_margin = 40, 60
    left_margin, right_margin = 40, 40
    usable_width = width - left_margin - right_margin
    page_usable_h = (height - top_margin) - bottom_margin
    line_h = 18
    y = height - top_margin

    total = len(records)

    def new_page():
        nonlocal y
        c.showPage()
        c.setFont(JAPANESE_FONT, 12)
        y = height - top_margin

    def draw_wrapped_lines(lines):
        nonlocal y
        for ln in lines:
            c.drawString(left_margin, y, ln)
            y -= line_h

    for idx, (_, row) in enumerate(records.iterrows(), start=1):
        q = safe_get(row, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"])

        choices = []
        for i in range(1, 6):
            v = safe_get(row, [f"é¸æŠè‚¢{i}"])
            if v:
                choices.append((i, v))

        ans = safe_get(row, ["æ­£è§£","æ­£ç­”","è§£ç­”","ç­”ãˆ"])
        cat = safe_get(row, ["ç§‘ç›®åˆ†é¡","åˆ†é¡","ç§‘ç›®"])
        code = safe_get(row, ["å•é¡Œç•ªå·ID","å•é¡Œç•ªå·"])

        # ç”»åƒã®äº‹å‰å–å¾—
        pil = None
        img_est_h = 0
        link_raw = safe_get(row, ["ãƒªãƒ³ã‚¯URL","ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯","URL"])
        if link_raw:
            try:
                image_url = convert_google_drive_link(link_raw)
                resp = requests.get(image_url, timeout=5)
                pil = Image.open(io.BytesIO(resp.content)).convert("RGB")
                iw, ih = pil.size
                scale = min(usable_width / iw, page_usable_h / ih, 1.0)
                nw, nh = iw * scale, ih * scale
                img_est_h = nh + 20
            except Exception:
                pil = None
                img_est_h = len(wrapped_lines("", "[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—]", usable_width, JAPANESE_FONT, 12)) * line_h

        # é«˜ã•è¦‹ç©ã‚Š
        est_h = 0
        q_lines = wrapped_lines("å•é¡Œæ–‡: ", q, usable_width, JAPANESE_FONT, 12)
        est_h += len(q_lines) * line_h
        choice_lines_list = []
        for i, v in choices:
            ls = wrapped_lines(f"é¸æŠè‚¢{i}: ", v, usable_width, JAPANESE_FONT, 12)
            choice_lines_list.append(ls)
            est_h += len(ls) * line_h
        est_h += img_est_h if img_est_h else 0
        ans_lines = wrapped_lines("æ­£è§£: ", ans, usable_width, JAPANESE_FONT, 12)
        cat_lines = wrapped_lines("åˆ†é¡: ", cat, usable_width, JAPANESE_FONT, 12)
        code_lines = wrapped_lines("å•é¡Œç•ªå·: ", code, usable_width, JAPANESE_FONT, 12)
        est_h += (len(ans_lines)+len(cat_lines)+len(code_lines)) * line_h + 20

        # ãƒšãƒ¼ã‚¸å…ˆé ­ã‚’å¿…ãšå•é¡Œæ–‡ã‹ã‚‰
        if y - est_h < bottom_margin:
            new_page()

        # æç”»
        draw_wrapped_lines(q_lines)
        for ls in choice_lines_list:
            draw_wrapped_lines(ls)

        if pil is not None:
            try:
                iw, ih = pil.size
                scale = min(usable_width / iw, page_usable_h / ih, 1.0)
                nw, nh = iw * scale, ih * scale
                if y - nh < bottom_margin:
                    new_page()
                remaining = y - bottom_margin
                if nh > remaining:
                    adj = remaining / nh
                    nw, nh = nw * adj, nh * adj
                img_io = io.BytesIO()
                pil.save(img_io, format="PNG")
                img_io.seek(0)
                img_reader = ImageReader(img_io)
                c.drawImage(img_reader, left_margin, y - nh, width=nw, height=nh, preserveAspectRatio=True, mask='auto')
                y -= nh + 20
            except Exception as e:
                err_lines = wrapped_lines("", f"[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {e}]", usable_width, JAPANESE_FONT, 12)
                draw_wrapped_lines(err_lines)
        else:
            if link_raw:
                draw_wrapped_lines(wrapped_lines("", "[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—]", usable_width, JAPANESE_FONT, 12))

        draw_wrapped_lines(ans_lines)
        draw_wrapped_lines(cat_lines)
        draw_wrapped_lines(code_lines)

        if y - 20 < bottom_margin:
            new_page()
        else:
            y -= 20

        if st.session_state.get("progress_on"):
            st.session_state["progress"].progress(min(idx / max(total, 1), 1.0))

    c.save()
    pdf_buffer.seek(0)
    return pdf_buffer.getvalue()

# ===== PDF ç”Ÿæˆ =====
if "pdf_bytes" not in st.session_state:
    st.session_state["pdf_bytes"] = None

if st.button("ğŸ–¨ï¸ PDFã‚’ä½œæˆï¼ˆç”»åƒä»˜ãï¼‰"):
    st.session_state["progress_on"] = True
    st.session_state["progress"] = st.progress(0.0)
    with st.spinner("PDFã‚’ä½œæˆä¸­â€¦"):
        st.session_state["pdf_bytes"] = create_pdf(df_filtered)
    st.session_state["progress_on"] = False
    st.success("âœ… PDFä½œæˆå®Œäº†ï¼")

if st.session_state["pdf_bytes"] is not None:
    st.download_button(
        label="ğŸ“„ ãƒ’ãƒƒãƒˆçµæœã‚’PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=st.session_state["pdf_bytes"],
        file_name=f"{file_prefix}.pdf",
        mime="application/pdf"
    )

# ===== ç”»é¢ã®ä¸€è¦§ï¼ˆæ­£è§£ã¯åˆæœŸéè¡¨ç¤ºï¼‰=====
st.markdown("### ğŸ” ãƒ’ãƒƒãƒˆã—ãŸå•é¡Œä¸€è¦§")
for i, (_, record) in enumerate(df_filtered.iterrows()):
    title = safe_get(record, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"])
    with st.expander(f"{i+1}. {title[:50]}..."):
        st.markdown("### ğŸ“ å•é¡Œæ–‡")
        st.write(title)

        st.markdown("### âœï¸ é¸æŠè‚¢")
        for j in range(1, 6):
            val = safe_get(record, [f"é¸æŠè‚¢{j}"])
            if val:
                st.write(f"- {val}")

        show_ans = st.checkbox("æ­£è§£ã‚’è¡¨ç¤ºã™ã‚‹", key=f"show_answer_{i}", value=False)
        if show_ans:
            st.markdown(f"**âœ… æ­£è§£:** {safe_get(record, ['æ­£è§£','æ­£ç­”','è§£ç­”','ç­”ãˆ'])}")
        else:
            st.markdown("**âœ… æ­£è§£:** |||ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§è¡¨ç¤ºï¼‰|||")

        st.markdown(f"**ğŸ“š åˆ†é¡:** {safe_get(record, ['ç§‘ç›®åˆ†é¡','åˆ†é¡','ç§‘ç›®'])}")
        code = safe_get(record, ["å•é¡Œç•ªå·ID","å•é¡Œç•ªå·"])
        if code:
            st.markdown(f"**ğŸ†” å•é¡Œç•ªå·:** {code}")

        link = safe_get(record, ["ãƒªãƒ³ã‚¯URL","ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯","URL"])
        if link:
            st.markdown(f"[ç”»åƒãƒªãƒ³ã‚¯ã¯ã“ã¡ã‚‰]({convert_google_drive_link(link)})")
        else:
            st.write("ï¼ˆç”»åƒãƒªãƒ³ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰")