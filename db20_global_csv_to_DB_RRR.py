import streamlit as st
import pandas as pd
import io
import csv
import requests
from PIL import Image
from datetime import datetime
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.pagesizes import A4
from reportlab.pdfbase.pdfmetrics import stringWidth
from reportlab.lib.utils import ImageReader
from pathlib import Path
from reportlab.pdfbase.cidfonts import UnicodeCIDFont
import re

# =========================================================
# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆIPAex ã‚’å„ªå…ˆã€ç„¡ã‘ã‚Œã°CIDãƒ•ã‚©ãƒ³ãƒˆã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
# =========================================================
def _setup_font():
    here = Path(__file__).parent
    candidates = [
        here / "fonts" / "IPAexGothic.ttf",
        here / "IPAexGothic.ttf",
        Path.cwd() / "fonts" / "IPAexGothic.ttf",
        Path.cwd() / "IPAexGothic.ttf",
    ]
    for p in candidates:
        if p.exists():
            pdfmetrics.registerFont(TTFont("Japanese", str(p)))
            return "Japanese"
    pdfmetrics.registerFont(UnicodeCIDFont("HeiseiKakuGo-W5"))
    return "HeiseiKakuGo-W5"

JAPANESE_FONT = _setup_font()

st.set_page_config(page_title="ğŸ” csvå•é¡Œdrop->æ¤œç´¢DB_txt_pdf_goodnote-csv_csvå‡ºåŠ›", layout="wide")
st.title("ğŸ” csvå•é¡Œdrop->æ¤œç´¢DB_txt_pdf_goodnote-csv_csvå‡ºåŠ›")

# =========================================================
# æ–‡å­—ãƒ»æ”¹è¡Œãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================================================
def _strip(s):
    if s is None:
        return ""
    return str(s).replace("\ufeff", "").strip()

def _norm_space(s):
    # åˆ—åç”¨ï¼šç©ºç™½ãƒ»æ”¹è¡Œãƒ»ã‚¿ãƒ–ã‚’é™¤å»ï¼ˆãƒ‡ãƒ¼ã‚¿æœ¬ä½“ã«ã¯ä½¿ã‚ãªã„ï¼‰
    return re.sub(r"[\u3000 \t\r\n]+", "", _strip(s))

def _normalize_newlines(text: str, newline: str = "\n") -> str:
    if text is None:
        return ""
    t = re.sub(r"\r\n|\r", "\n", str(text))
    if newline == "\r\n":
        t = t.replace("\n", "\r\n")
    return t

def _safe_filename(s: str) -> str:
    s = _strip(s)
    s = re.sub(r'[\\/:*?"<>|]+', "_", s)
    s = re.sub(r"\s+", "_", s)
    return s[:80] if s else "search"

# =========================================================
# åˆ—åæ­£è¦åŒ–
# =========================================================
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # åˆ—åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆåˆ—åã®ã¿ï¼‰
    df.rename(columns={c: _norm_space(c) for c in df.columns}, inplace=True)

    # ã‚ˆãã‚ã‚‹åˆ¥å â†’ æ­£å¼å
    alias = {
        "å•é¡Œæ–‡":  ["è¨­å•", "å•é¡Œ", "æœ¬æ–‡"],
        "é¸æŠè‚¢1": ["é¸æŠè‚¢ï¼¡","é¸æŠè‚¢a","A","ï½"],
        "é¸æŠè‚¢2": ["é¸æŠè‚¢ï¼¢","é¸æŠè‚¢b","B","ï½‚"],
        "é¸æŠè‚¢3": ["é¸æŠè‚¢ï¼£","é¸æŠè‚¢c","C","ï½ƒ"],
        "é¸æŠè‚¢4": ["é¸æŠè‚¢ï¼¤","é¸æŠè‚¢d","D","ï½„"],
        "é¸æŠè‚¢5": ["é¸æŠè‚¢ï¼¥","é¸æŠè‚¢e","E","ï½…"],
        "æ­£è§£":    ["è§£ç­”","ç­”ãˆ","ans","answer","æ­£ç­”"],
        "ç§‘ç›®åˆ†é¡": ["åˆ†é¡","ç§‘ç›®","ã‚«ãƒ†ã‚´ãƒª","ã‚«ãƒ†ã‚´ãƒªãƒ¼","åˆ†é‡"],
        "ãƒªãƒ³ã‚¯URL": ["ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯","ç”»åƒLink","URL","url"],
        "å•é¡Œç•ªå·ID": ["å•é¡Œç•ªå·", "è­˜åˆ¥ç•ªå·", "ID", "ç•ªå·"],
    }
    colset = set(df.columns)
    for canon, cands in alias.items():
        if canon in colset:
            continue
        for c in cands:
            if c in colset:
                df.rename(columns={c: canon}, inplace=True)
                colset.add(canon)
                break

    return df

def safe_get(row: pd.Series | dict, keys, default=""):
    if isinstance(row, pd.Series):
        row = row.to_dict()
    for k in keys:
        if k in row:
            v = row.get(k)
            try:
                if pd.isna(v):
                    continue
            except Exception:
                pass
            s = str(v) if v is not None else ""
            s = s.strip()
            if s != "":
                return s
    return default

def ensure_output_columns(df: pd.DataFrame) -> pd.DataFrame:
    need = [
        "å•é¡Œæ–‡","é¸æŠè‚¢1","é¸æŠè‚¢2","é¸æŠè‚¢3","é¸æŠè‚¢4","é¸æŠè‚¢5",
        "æ­£è§£","ç§‘ç›®åˆ†é¡","å•é¡Œç•ªå·ID","ãƒªãƒ³ã‚¯URL"
    ]
    out = df.copy()
    for c in need:
        if c not in out.columns:
            out[c] = ""
    return out

# =========================================================
# â˜… æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’ã€Œæ—¥æœ¬èªã¨ã—ã¦è‡ªç„¶ã«èª­ã‚ã‚‹ã‚‚ã®ã€ã«å¯„ã›ã‚‹ãƒ‡ã‚³ãƒ¼ãƒ‰
# =========================================================
def _decode_best_effort(raw: bytes) -> str:
    candidates = ["utf-8-sig", "utf-8", "cp932", "shift_jis", "euc_jp"]

    for enc in candidates:
        try:
            return raw.decode(enc)
        except Exception:
            pass

    best_text = None
    best_score = None
    for enc in ["cp932", "shift_jis", "euc_jp", "utf-8-sig", "utf-8"]:
        try:
            t = raw.decode(enc, errors="replace")
        except Exception:
            continue

        rep = t.count("\ufffd")
        jp = sum(
            1 for ch in t
            if ("\u3040" <= ch <= "\u30ff") or ("\u4e00" <= ch <= "\u9fff")
        )
        score = rep * 1000 - jp
        if best_score is None or score < best_score:
            best_score = score
            best_text = t

    if best_text is not None:
        return best_text

    return raw.decode("utf-8", errors="ignore")

# =========================================================
# â˜… ãƒ˜ãƒƒãƒ€æœ‰ç„¡åˆ¤å®šï¼†è‡ªå‹•ãƒ˜ãƒƒãƒ€ä»˜ä¸ï¼ˆâ†â‘¢å¯¾ç­–ã®æœ¬ä½“ï¼‰
# =========================================================
def _looks_like_header(cols: list[str]) -> bool:
    joined = "".join(cols)
    header_tokens = ["å•é¡Œæ–‡", "è¨­å•", "é¸æŠè‚¢", "æ­£è§£", "è§£ç­”", "åˆ†é¡", "ç§‘ç›®", "ãƒªãƒ³ã‚¯", "URL", "ID", "ç•ªå·"]
    return any(tok in joined for tok in header_tokens)

def _default_header_by_ncol(ncol: int) -> list[str]:
    # ã¾ãšã€Œå•é¡Œæ–‡ + é¸æŠè‚¢1-5ã€ã¯å…±é€šã¨ã—ã¦å¯„ã›ã‚‹
    base = ["å•é¡Œæ–‡", "é¸æŠè‚¢1", "é¸æŠè‚¢2", "é¸æŠè‚¢3", "é¸æŠè‚¢4", "é¸æŠè‚¢5"]

    # â‘¢ã¯ ncol=8 ã§ã€Œæ­£è§£(ç©º)ã€ã€Œç§‘ç›®åˆ†é¡ã€
    if ncol == 8:
        return base + ["æ­£è§£", "ç§‘ç›®åˆ†é¡"]

    # ã‚ˆãã‚ã‚‹æƒ³å®šï¼š+ æ­£è§£ + ç§‘ç›®åˆ†é¡ + å•é¡Œç•ªå·ID + ãƒªãƒ³ã‚¯URLï¼ˆè¨ˆ10ï¼‰
    if ncol == 10:
        return base + ["æ­£è§£", "ç§‘ç›®åˆ†é¡", "å•é¡Œç•ªå·ID", "ãƒªãƒ³ã‚¯URL"]

    # 9åˆ—ï¼šãƒªãƒ³ã‚¯ç„¡ã— or å•é¡Œç•ªå·ç„¡ã—ç­‰ã®ã‚±ãƒ¼ã‚¹
    if ncol == 9:
        # ã€Œæ­£è§£ãƒ»ç§‘ç›®åˆ†é¡ãƒ»å•é¡Œç•ªå·IDã€ã¾ã§ã‚’å„ªå…ˆ
        return base + ["æ­£è§£", "ç§‘ç›®åˆ†é¡", "å•é¡Œç•ªå·ID"]

    # 7åˆ—ï¼šæ­£è§£ç„¡ã—ã§åˆ†é¡ã¾ã§ ãªã©
    if ncol == 7:
        return base + ["ç§‘ç›®åˆ†é¡"]

    # ãã‚Œä»¥å¤–ï¼šã¨ã‚Šã‚ãˆãšcol_1..ã§ä½œã‚‹ï¼ˆå£Šã‚Œãªã„ã“ã¨å„ªå…ˆï¼‰
    return [f"col{i+1}" for i in range(ncol)]

# =========================================================
# â˜… CSVèª­ã¿è¾¼ã¿ï¼ˆãƒ˜ãƒƒãƒ€è£œæ­£ï¼‹åˆ—æ•°è£œæ­£ï¼‹ãƒ˜ãƒƒãƒ€ç„¡ã—å¯¾å¿œï¼‰
# =========================================================
def read_csv_safely_with_column_fix(uploaded_file) -> pd.DataFrame:
    raw = uploaded_file.getvalue()
    text = _decode_best_effort(raw)

    lines = text.splitlines()
    if not lines:
        return pd.DataFrame()

    # 1è¡Œç›®ã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚†ã‚‰ãå¯¾ç­–ï¼ˆãƒ˜ãƒƒãƒ€å€™è£œã®ã¿ï¼‰
    first_line = lines[0].replace("ã€", ",")
    first_cols = next(csv.reader([first_line]))

    has_header = _looks_like_header([c.strip() for c in first_cols])

    if has_header:
        header = [h.strip().replace("\ufeff", "") for h in first_cols]
        data_lines = lines[1:]
    else:
        # â˜…ãƒ˜ãƒƒãƒ€ç„¡ã—ï¼š1è¡Œç›®ã¯ãƒ‡ãƒ¼ã‚¿
        ncol = len(first_cols)
        header = _default_header_by_ncol(ncol)
        data_lines = lines  # 1è¡Œç›®ã‹ã‚‰å…¨éƒ¨ãƒ‡ãƒ¼ã‚¿

    ncol = len(header)

    fixed_rows = []
    reader = csv.reader(data_lines)
    for row in reader:
        if not row or all((c.strip() == "" for c in row)):
            continue

        # åˆ—ãŒå¤šã™ãã‚‹ï¼šä½™ã‚Šã‚’å…ˆé ­åˆ—ã¸å¸å
        while len(row) > ncol:
            row[0] = row[0] + "," + row[1]
            del row[1]

        # åˆ—ãŒè¶³ã‚Šãªã„ï¼šå³ã‚’ç©ºã§åŸ‹ã‚ã‚‹
        if len(row) < ncol:
            row = row + [""] * (ncol - len(row))

        fixed_rows.append(row)

    df = pd.DataFrame(fixed_rows, columns=header).fillna("")
    return df

# =========================================================
# å…¥åŠ›CSVï¼ˆDrag & Dropï¼‰
# =========================================================
uploaded = st.file_uploader("CSVã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ï¼ˆã¾ãŸã¯é¸æŠï¼‰", type=["csv"])
if uploaded is None:
    st.info("ä»»æ„ã®CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆãƒ˜ãƒƒãƒ€ã‚ã‚Šæ¨å¥¨ï¼‰ã€‚")
    st.stop()

df = read_csv_safely_with_column_fix(uploaded)
df = normalize_columns(df)

# URLåˆ—ã ã‘ã®CSVã§ã‚‚ã€Œãƒªãƒ³ã‚¯URLã€ã«å¯„ã›ã‚‹
if "ãƒªãƒ³ã‚¯URL" not in df.columns and "URL" in df.columns:
    df.rename(columns={"URL": "ãƒªãƒ³ã‚¯URL"}, inplace=True)

# =========================================================
# æ¤œç´¢
# =========================================================
query = st.text_input("æ¤œç´¢èªï¼ˆä¾‹ï¼šã©ã‚Œã‹ / ã‚¨ãƒŠãƒ¡ãƒ«è³ª / 1459ï¼‰")
st.caption("ğŸ’¡ `&` ã§ANDæ¤œç´¢ï¼ˆä¾‹: ãƒ¬ã‚¸ãƒ³ & ç†å·¥ï¼‰")

if not query:
    st.stop()

keywords = [kw.strip() for kw in query.split("&") if kw.strip()]

def row_text(r: pd.Series) -> str:
    vals = []
    for v in r.values:
        if v is None:
            continue
        s = str(v)
        if s.strip() == "":
            continue
        vals.append(s)
    return " ".join(vals)

def match_all_keywords(row: pd.Series) -> bool:
    text = row_text(row).casefold()
    return all(kw.casefold() in text for kw in keywords)

df_filtered = df[df.apply(match_all_keywords, axis=1)].reset_index(drop=True)
st.info(f"{len(df_filtered)}ä»¶ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸ")

timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
file_prefix = f"{_safe_filename(query)}_{timestamp}"

# =========================================================
# CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =========================================================
csv_buffer = io.StringIO()
ensure_output_columns(df_filtered).to_csv(csv_buffer, index=False)
st.download_button(
    label="ğŸ“¥ ãƒ’ãƒƒãƒˆçµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=csv_buffer.getvalue(),
    file_name=f"{file_prefix}.csv",
    mime="text/csv"
)

# =========================================================
# GoodNotes ç”¨ CSVï¼ˆFront/Backï¼‰
# =========================================================
def _gn_clean(s: str) -> str:
    return _strip(s).replace("ã€€", "")

def _gn_make_front_back(row: pd.Series,
                        numbering: str = "ABC",
                        add_labels: bool = True,
                        add_meta: bool = True) -> tuple[str, str]:
    q = _gn_clean(safe_get(row, ["å•é¡Œæ–‡"]))
    choices = [
        _gn_clean(safe_get(row, ["é¸æŠè‚¢1"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢2"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢3"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢4"])),
        _gn_clean(safe_get(row, ["é¸æŠè‚¢5"])),
    ]
    labels = ["A","B","C","D","E"] if numbering == "ABC" else ["1","2","3","4","5"]
    choice_lines = [f"{labels[i]}. {_normalize_newlines(txt)}" for i, txt in enumerate(choices) if txt]

    front = _normalize_newlines(q)
    if choice_lines:
        front = front + "\n\n" + "\n".join(choice_lines)

    ans = _gn_clean(safe_get(row, ["æ­£è§£"]))
    back = f"æ­£è§£: {ans}" if add_labels else ans

    if add_meta:
        cat = _gn_clean(safe_get(row, ["ç§‘ç›®åˆ†é¡"]))
        code = _gn_clean(safe_get(row, ["å•é¡Œç•ªå·ID"]))
        extra = "\n".join([s for s in (cat, code) if s])
        if extra:
            back = back + "\n\n" + _normalize_newlines(extra)

    back = _normalize_newlines(back)
    return front, back

def dataframe_to_goodnotes_bytes(df_in: pd.DataFrame) -> bytes:
    base = ensure_output_columns(df_in)
    fronts, backs = [], []
    for _, row in base.iterrows():
        f, b = _gn_make_front_back(row)
        fronts.append(f); backs.append(b)
    out = pd.DataFrame({"Front": fronts, "Back": backs})
    for c in out.columns:
        out[c] = out[c].map(lambda v: _normalize_newlines(v, "\n"))
    buf = io.StringIO()
    buf.write("\ufeff")
    out.to_csv(buf, index=False, lineterminator="\n")
    return buf.getvalue().encode("utf-8")

st.download_button(
    label="ğŸ“¥ GoodNotesç”¨CSVï¼ˆFront/Backï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=dataframe_to_goodnotes_bytes(df_filtered),
    file_name=f"{file_prefix}_goodnotes.csv",
    mime="text/csv",
)

# =========================================================
# TXT / PDF å…±é€š
# =========================================================
def convert_google_drive_link(url):
    if "drive.google.com" in url and "/file/d/" in url:
        try:
            file_id = url.split("/file/d/")[1].split("/")[0]
            return f"https://drive.google.com/uc?export=view&id={file_id}"
        except Exception:
            return url
    return url

def wrap_text(text: str, max_width: float, font_name: str, font_size: int):
    s = "" if text is None else str(text)
    if s == "":
        return [""]
    lines, buf = [], ""
    for ch in s:
        if stringWidth(buf + ch, font_name, font_size) <= max_width:
            buf += ch
        else:
            lines.append(buf)
            buf = ch
    if buf:
        lines.append(buf)
    return lines

def wrapped_lines(prefix: str, value: str, usable_width: float, font: str, size: int):
    return wrap_text(f"{prefix}{value}", usable_width, font, size)

def format_record_to_text(row: pd.Series) -> str:
    q = safe_get(row, ["å•é¡Œæ–‡"])
    parts = [f"å•é¡Œæ–‡: {q}"]
    for i in range(1, 6):
        choice = safe_get(row, [f"é¸æŠè‚¢{i}"])
        if choice:
            parts.append(f"é¸æŠè‚¢{i}: {choice}")
    parts.append(f"æ­£è§£: {safe_get(row, ['æ­£è§£'])}")
    parts.append(f"åˆ†é¡: {safe_get(row, ['ç§‘ç›®åˆ†é¡'])}")
    code = safe_get(row, ["å•é¡Œç•ªå·ID"])
    if code:
        parts.append(f"å•é¡Œç•ªå·: {code}")
    link = safe_get(row, ["ãƒªãƒ³ã‚¯URL"])
    if link:
        parts.append(f"ç”»åƒãƒªãƒ³ã‚¯: {convert_google_drive_link(link)}ï¼ˆPDFã«ç”»åƒè¡¨ç¤ºï¼‰")
    return "\n".join(parts)

# =========================================================
# TXT ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =========================================================
txt_buffer = io.StringIO()
for _, row in df_filtered.iterrows():
    txt_buffer.write(format_record_to_text(row))
    txt_buffer.write("\n\n" + "-"*40 + "\n\n")
st.download_button(
    label="ğŸ“„ ãƒ’ãƒƒãƒˆçµæœã‚’TEXTãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=txt_buffer.getvalue(),
    file_name=f"{file_prefix}.txt",
    mime="text/plain"
)

# =========================================================
# PDF ä½œæˆ
# =========================================================
def create_pdf(records: pd.DataFrame):
    pdf_buffer = io.BytesIO()
    c = canvas.Canvas(pdf_buffer, pagesize=A4)
    c.setFont(JAPANESE_FONT, 12)
    width, height = A4

    top_margin, bottom_margin = 40, 60
    left_margin, right_margin = 40, 40
    usable_width = width - left_margin - right_margin
    page_usable_h = (height - top_margin) - bottom_margin
    line_h = 18
    y = height - top_margin

    def new_page():
        nonlocal y
        c.showPage()
        c.setFont(JAPANESE_FONT, 12)
        y = height - top_margin

    def draw_wrapped_lines(lines):
        nonlocal y
        for ln in lines:
            c.drawString(left_margin, y, ln)
            y -= line_h

    for _, row in records.iterrows():
        q = safe_get(row, ["å•é¡Œæ–‡"])

        choices = []
        for i in range(1, 6):
            v = safe_get(row, [f"é¸æŠè‚¢{i}"])
            if v:
                choices.append((i, v))

        ans = safe_get(row, ["æ­£è§£"])
        cat = safe_get(row, ["ç§‘ç›®åˆ†é¡"])
        code = safe_get(row, ["å•é¡Œç•ªå·ID"])

        pil = None
        img_est_h = 0
        link_raw = safe_get(row, ["ãƒªãƒ³ã‚¯URL"])
        if link_raw:
            try:
                image_url = convert_google_drive_link(link_raw)
                resp = requests.get(image_url, timeout=5)
                pil = Image.open(io.BytesIO(resp.content)).convert("RGB")
                iw, ih = pil.size
                scale = min(usable_width / iw, page_usable_h / ih, 1.0)
                nw, nh = iw * scale, ih * scale
                img_est_h = nh + 20
            except Exception:
                pil = None
                img_est_h = len(wrapped_lines("", "[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—]", usable_width, JAPANESE_FONT, 12)) * line_h

        est_h = 0
        q_lines = wrapped_lines("å•é¡Œæ–‡: ", q, usable_width, JAPANESE_FONT, 12)
        est_h += len(q_lines) * line_h
        choice_lines_list = []
        for i, v in choices:
            ls = wrapped_lines(f"é¸æŠè‚¢{i}: ", v, usable_width, JAPANESE_FONT, 12)
            choice_lines_list.append(ls)
            est_h += len(ls) * line_h
        est_h += img_est_h if img_est_h else 0
        ans_lines = wrapped_lines("æ­£è§£: ", ans, usable_width, JAPANESE_FONT, 12)
        cat_lines = wrapped_lines("åˆ†é¡: ", cat, usable_width, JAPANESE_FONT, 12)
        code_lines = wrapped_lines("å•é¡Œç•ªå·: ", code, usable_width, JAPANESE_FONT, 12)
        est_h += (len(ans_lines) + len(cat_lines) + len(code_lines)) * line_h + 20

        if y - est_h < bottom_margin:
            new_page()

        draw_wrapped_lines(q_lines)
        for ls in choice_lines_list:
            draw_wrapped_lines(ls)

        if pil is not None:
            try:
                iw, ih = pil.size
                scale = min(usable_width / iw, page_usable_h / ih, 1.0)
                nw, nh = iw * scale, ih * scale
                if y - nh < bottom_margin:
                    new_page()
                remaining = y - bottom_margin
                if nh > remaining:
                    adj = remaining / nh
                    nw, nh = nw * adj, nh * adj
                img_io = io.BytesIO()
                pil.save(img_io, format="PNG")
                img_io.seek(0)
                img_reader = ImageReader(img_io)
                c.drawImage(img_reader, left_margin, y - nh, width=nw, height=nh,
                            preserveAspectRatio=True, mask='auto')
                y -= nh + 20
            except Exception as e:
                err_lines = wrapped_lines("", f"[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {e}]", usable_width, JAPANESE_FONT, 12)
                draw_wrapped_lines(err_lines)
        else:
            if link_raw:
                draw_wrapped_lines(wrapped_lines("", "[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—]", usable_width, JAPANESE_FONT, 12))

        draw_wrapped_lines(ans_lines)
        draw_wrapped_lines(cat_lines)
        draw_wrapped_lines(code_lines)

        if y - 20 < bottom_margin:
            new_page()
        else:
            y -= 20

    c.save()
    pdf_buffer.seek(0)
    return pdf_buffer.getvalue()

if "pdf_bytes" not in st.session_state:
    st.session_state["pdf_bytes"] = None

if st.button("ğŸ–¨ï¸ PDFã‚’ä½œæˆï¼ˆç”»åƒä»˜ãï¼‰"):
    with st.spinner("PDFã‚’ä½œæˆä¸­â€¦"):
        st.session_state["pdf_bytes"] = create_pdf(df_filtered)
    st.success("âœ… PDFä½œæˆå®Œäº†ï¼")

if st.session_state["pdf_bytes"] is not None:
    st.download_button(
        label="ğŸ“„ ãƒ’ãƒƒãƒˆçµæœã‚’PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=st.session_state["pdf_bytes"],
        file_name=f"{file_prefix}.pdf",
        mime="application/pdf"
    )

# =========================================================
# ç”»é¢ã®ä¸€è¦§ï¼ˆå•é¡Œæ–‡æœ«å°¾ã®è­˜åˆ¥ç•ªå·ã‚’åˆ‡ã‚‰ãªã„ï¼‰
# =========================================================
st.markdown("### ğŸ” ãƒ’ãƒƒãƒˆã—ãŸå•é¡Œä¸€è¦§")
for i, (_, record) in enumerate(df_filtered.iterrows()):
    title = safe_get(record, ["å•é¡Œæ–‡"])
    with st.expander(f"{i+1}. {title}"):
        st.markdown("### ğŸ“ å•é¡Œæ–‡")
        st.write(title)

        st.markdown("### âœï¸ é¸æŠè‚¢")
        for j in range(1, 6):
            val = safe_get(record, [f"é¸æŠè‚¢{j}"])
            if val:
                st.write(f"- {val}")

        show_ans = st.checkbox("æ­£è§£ã‚’è¡¨ç¤ºã™ã‚‹", key=f"show_answer_{i}", value=False)
        if show_ans:
            st.markdown(f"**âœ… æ­£è§£:** {safe_get(record, ['æ­£è§£'])}")
        else:
            st.markdown("**âœ… æ­£è§£:** |||ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§è¡¨ç¤ºï¼‰|||")

        st.markdown(f"**ğŸ“š åˆ†é¡:** {safe_get(record, ['ç§‘ç›®åˆ†é¡'])}")

        code = safe_get(record, ["å•é¡Œç•ªå·ID"])
        if code:
            st.markdown(f"**ğŸ†” å•é¡Œç•ªå·:** {code}")

        link = safe_get(record, ["ãƒªãƒ³ã‚¯URL"])
        if link:
            st.markdown(f"[ç”»åƒãƒªãƒ³ã‚¯ã¯ã“ã¡ã‚‰]({convert_google_drive_link(link)})")
        else:
            st.write("ï¼ˆç”»åƒãƒªãƒ³ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰")
